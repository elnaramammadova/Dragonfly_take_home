{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "107e162f",
   "metadata": {},
   "source": [
    "# Junior Data Scientist Take-Home Task: Product Catalogue Creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96d815",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05391e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822290d",
   "metadata": {},
   "source": [
    "## 1. Data Ingestation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CSV files\n",
    "bd_tech = pd.read_csv('data/bd_technologies.csv')\n",
    "ts_tech = pd.read_csv('data/ts_technologies.csv')\n",
    "\n",
    "# reset the index of both DataFrames\n",
    "bd_tech.reset_index(drop=True, inplace=True)\n",
    "ts_tech.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da2e61",
   "metadata": {},
   "source": [
    "## 2.1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first few rows of each DataFrame\n",
    "display(bd_tech.head()), display(ts_tech.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63933340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the column names of each DataFrame\n",
    "display(bd_tech.columns, ts_tech.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48705b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics for each DataFrame\n",
    "display(bd_tech.describe(), ts_tech.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the general information about each DataFrame\n",
    "bd_tech.info(verbose=True), ts_tech.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f845058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate data quality report\n",
    "def data_quality_report(df):\n",
    "    return pd.DataFrame({\n",
    "        \"Missing Values\": df.isnull().sum(),\n",
    "        \"Percentage Missing\": (df.isnull().sum() / len(df)) * 100,\n",
    "        \"Data Type\": df.dtypes\n",
    "    })\n",
    "# Generate data quality reports\n",
    "bd_quality_report = data_quality_report(bd_tech)\n",
    "ts_quality_report = data_quality_report(ts_tech)\n",
    "\n",
    "display(bd_quality_report), display(ts_quality_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a176699",
   "metadata": {},
   "source": [
    "**Initial Data Quality Issues:**\n",
    "\n",
    "> bd_technologies.csv\n",
    "\n",
    "- Major missing values in the headquarters column (~43.8% missing).\n",
    "\n",
    "- Minimal missing values in seller_website and categories.\n",
    " * Also its important to note that these values are not accurate until the inconsistencies with data types and missing values are addressed. \n",
    " \n",
    "> ts_technologies.csv\n",
    "\n",
    "- Significant missing values in url (~76.8%) and description (~73.9%).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afce693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display duplicates in each DataFrame\n",
    "def display_duplicates(df):\n",
    "    duplicates = df[df.duplicated()]\n",
    "    if not duplicates.empty:\n",
    "        print(f\"Duplicates found:\\n{duplicates}\")\n",
    "    else:\n",
    "        print(\"No duplicates found.\")\n",
    "display_duplicates(bd_tech)\n",
    "display_duplicates(ts_tech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830118d8",
   "metadata": {},
   "source": [
    "## 2.2. Data Cleaning\n",
    "\n",
    "## 2.2.1. Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74289b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace field that's entirely space (or empty) with NaN\n",
    "ts_tech = ts_tech.replace(r'^\\s*$', np.nan, regex=True)\n",
    "bd_tech = bd_tech.replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5939463",
   "metadata": {},
   "source": [
    "### 2.2.2. Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9764736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all column data types to string for consistency in db_tech\n",
    "def convert_to_string(df):\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype('string')\n",
    "    return df\n",
    "bd_tech = convert_to_string(bd_tech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cf2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_tech.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the columns 1 to 9 in ts_tech to string\n",
    "def convert_ts_tech_to_string(df):\n",
    "    for col in df.columns[1:9]:\n",
    "        df[col] = df[col].astype('string')\n",
    "    return df\n",
    "ts_tech = convert_ts_tech_to_string(ts_tech)\n",
    "ts_tech.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548cf22",
   "metadata": {},
   "source": [
    "### 2.2.3. Standardize the URL \n",
    "\n",
    "While the primary identifiers are obviously product name and description, we can also consider URLs and Seller Websites which are often unique and can strongly confirm product matches. Therefore it is essential we have standardized URLs (removal of protocols, www and trailing slashes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd78d336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the url and seller_website columns by removing protocols, wwww and trailing slashes\n",
    "def standardize_urls(df, url_col):\n",
    "    df[url_col] = df[url_col].str.lower()  # convert to lowercase\n",
    "    df[url_col] = df[url_col].str.replace(r'^https?://', '', regex=True)  # remove http/https\n",
    "    df[url_col] = df[url_col].str.replace(r'^www\\.', '', regex=True)  # remove www.\n",
    "    df[url_col] = df[url_col].str.rstrip('/')  # remove trailing slashes\n",
    "    return df\n",
    "bd_tech = standardize_urls(bd_tech, 'seller_website')\n",
    "ts_tech = standardize_urls(ts_tech, 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b804291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 20 random entries from the seller_website column for quality control\n",
    "bd_tech['seller_website'].sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe623e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_tech['url'].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e58baf",
   "metadata": {},
   "source": [
    "### 2.2.4. Clean the categories column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457b093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the categories column in bd_tech - removing brackets and quotes\n",
    "bd_tech['categories'] = (\n",
    "    bd_tech['categories']\n",
    "    .str.replace(r'[\\[\\]\"]', '', regex=True)           # Remove brackets and quotes\n",
    "    .str.replace(r',\\s*', ', ', regex=True)            # Ensure single space after comma\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d604fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_tech.categories.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30db4d3",
   "metadata": {},
   "source": [
    "### 2.2.5. Clean the software_product_id and parent_category_slug column\n",
    "\n",
    "`software_product_id` column in `bd_tech` and `parent_category_slug` column in `ts_tech` have string values seperated by hyphens replace them with space instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d669e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyphens(column):\n",
    "    # Replace hyphens with spaces in the specified column\n",
    "    return column.str.replace(r'(?<=[A-Za-z])-(?=[A-Za-z])', ' ', regex=True)\n",
    "\n",
    "# Apply the function to the 'software_product_id' column in bd_tech \n",
    "bd_tech['software_product_id'] = remove_hyphens(bd_tech['software_product_id'])\n",
    "\n",
    "# Apply the function to the 'slug', 'category_slug' and 'parent_category_slug' column in ts_tech\n",
    "ts_tech['slug'] = remove_hyphens(ts_tech['slug'])\n",
    "ts_tech['category_slug'] = remove_hyphens(ts_tech['category_slug'])\n",
    "ts_tech['parent_category_slug'] = remove_hyphens(ts_tech['parent_category_slug'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1248a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the cleaned 'software_product_id' column in bd_tech\n",
    "bd_tech['software_product_id'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429036ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 5 entries from the 'slug', 'category_slug', 'parent_category_slug' columns in ts_tech\n",
    "ts_tech[['slug', 'category_slug', 'parent_category_slug']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abd469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 'category' and 'parent_category' in ts_tech into a single column called 'categories' and drop the original two columns\n",
    "ts_tech_clean = ts_tech.copy()\n",
    "ts_tech_clean['categories_ts'] = ts_tech_clean['category'] + ', ' + ts_tech_clean['parent_category']\n",
    "ts_tech_clean.drop(columns=['category', 'parent_category', 'parent_category_slug', 'category_slug', 'slug'], inplace=True) \n",
    "# remove leading and trailing spaces from the 'categories' column in ts_tech\n",
    "ts_tech_clean['categories_ts'] = ts_tech_clean['categories_ts'].str.strip()\n",
    "\n",
    "# combine main_category and categories in bd_tech into a single column called 'categories' and drop the original two columns\n",
    "bd_tech_clean = bd_tech.copy()\n",
    "bd_tech_clean['categories_bd'] = bd_tech_clean['main_category'] + ', ' + bd_tech_clean['categories']\n",
    "bd_tech_clean.drop(columns=['main_category', 'categories', 'software_product_id'], inplace=True)\n",
    "# remove leading and trailing spaces from the 'categories' column in bd_tech\n",
    "bd_tech_clean['categories_bd'] = bd_tech_clean['categories_bd'].str.strip()\n",
    "# display the first few rows of each DataFrame after cleaning\n",
    "display(bd_tech_clean.head()), display(ts_tech_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3485448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_tech_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a898cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_tech_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada4d088",
   "metadata": {},
   "source": [
    "## 3. Product Deduplicaton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9f651",
   "metadata": {},
   "source": [
    "### 3.1. Data Engineering\n",
    "\n",
    "Here we will standardize our text. This ensures that data across records follows the same format, which is crucial for reliable comparison. urls have already been standardized. Next we clean text by converting to lowercase, removing punctutations, extra whitespaces and stopwords, and lastly applying lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfbb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "bd_tech_processed = bd_tech_clean.copy()\n",
    "ts_tech_processed = ts_tech_clean.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cleaning function\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Define preprocessing function for combined columns\n",
    "def preprocess(df, columns):\n",
    "    return df[columns].fillna('').agg(' '.join, axis=1).apply(clean_text)\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "bd_tech_processed['combined'] = preprocess(bd_tech_processed, ['product_name', 'description', 'seller_website', 'categories_bd'])\n",
    "ts_tech_processed['combined'] = preprocess(ts_tech_processed, ['name', 'description', 'url', 'categories_ts'])\n",
    "\n",
    "# Display the first few results\n",
    "bd_tech_processed[['product_name', 'combined']].head(), ts_tech_processed[['name', 'combined']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebca35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the cleaned DataFrames to CSV files\n",
    "bd_tech_processed.to_csv('data/bd_technologies_cleaned.csv', index=False)\n",
    "ts_tech_processed.to_csv('data/ts_technologies_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eab9b9",
   "metadata": {},
   "source": [
    "### 3.2. Fuzzy Matching (Rapid Initial Matching)\n",
    "\n",
    "Fuzzy Matching is a quick tool for calculating similarities between strings to help identify values that are \"close enough\". This method allows variations and/or inconsistencies in data (i.e., typos, different spelling) to be considered similar. We calculate the Levenshtein distance between our strings with a threshold of 0.85. If the similarity score exceeds the threshold, the data records are considered a fuzzy match and can be linked. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d5beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned DataFrames\n",
    "bd_tech_processed = pd.read_csv('data/bd_technologies_cleaned.csv')\n",
    "ts_tech_processed = pd.read_csv('data/ts_technologies_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f60577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# def fuzzy_match(ts_text, bd_choices, threshold=85):\n",
    "#     match, score = process.extractOne(ts_text, bd_choices)\n",
    "#     return (match, score) if score >= threshold else (None, score)\n",
    "\n",
    "# ts_tech_processed['fuzzy_match'] = ts_tech_processed['combined'].apply(lambda x: fuzzy_match(x, bd_tech_processed['combined'].tolist()))\n",
    "\n",
    "# fuzzywuzzy was taking too long to run, so we will switch to thefuzz instead \n",
    "\n",
    "# from thefuzz import fuzz, process\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # enable tqdm for pandas\n",
    "# tqdm.pandas()\n",
    "\n",
    "# def fuzzy_match_fast(ts_text, bd_choices, threshold=85):\n",
    "#     match, score = process.extractOne(ts_text, bd_choices, scorer=fuzz.token_set_ratio)\n",
    "#     return (match, score) if score >= threshold else (None, score)\n",
    "\n",
    "# # Apply to TS dataset with progress bar\n",
    "# ts_tech_processed['fuzzy_match'] = ts_tech_processed['combined'].progress_apply(\n",
    "#     lambda x: fuzzy_match_fast(x, bd_tech_processed['combined'].tolist())\n",
    "# )\n",
    "# The above code was still taking too long, so we will try using RapidFuzz for faster fuzzy matching\n",
    "\n",
    "# Use RapidFuzz for faster fuzzy matching\n",
    "# from tqdm import tqdm\n",
    "# from rapidfuzz import fuzz, process\n",
    "\n",
    "# # enable tqdm for pandas\n",
    "# tqdm.pandas()\n",
    "\n",
    "# def fuzzy_match_fast(ts_text, bd_choices, threshold=85):\n",
    "#     match = process.extractOne(ts_text, bd_choices, scorer=fuzz.token_set_ratio)\n",
    "#     return match if match else (None, 0)\n",
    "\n",
    "# # Apply to TS dataset with progress bar\n",
    "# ts_tech_processed['fuzzy_match'] = ts_tech_processed['combined'].progress_apply(\n",
    "#     lambda x: fuzzy_match_fast(x, bd_tech_processed['combined'].tolist())\n",
    "# )\n",
    "\n",
    "# Even this would take 4 hrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766327de",
   "metadata": {},
   "source": [
    "Both Fuzzy Matching methods took longer than 8 hrs to complete. We will try to speed up the fuzzy matching process by applying below methods: \n",
    "- Limit to top 10 cadidates with TF-IDF Pre-Filtering by cosine similarity\n",
    "- Apply fuzzy matching only on those \n",
    "- use RapidFuzz instead of thefuzz and fuzzywuzzy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34a97b",
   "metadata": {},
   "source": [
    "This method will provide a semantic matching beyond simple string similarity. \n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency) and cosine similarity are used together to measure the similarity between text documents. TF-IDF converts text into numerical vectors, while cosine similarity calculates the similarity between those vectors. A higher cosine similarity value indicates greater similarity between documents. \n",
    "1. TF-IDF:\n",
    "Term Frequency (TF): Measures how often a term appears in a document. \n",
    "Inverse Document Frequency (IDF): Measures how rare a term is across a corpus of documents. \n",
    "TF-IDF Value: The product of TF and IDF, weighting terms based on their importance in a specific document and the entire corpus. \n",
    "Vector Representation: TF-IDF converts each document into a numerical vector, where each dimension represents a term and the value in that dimension is the term's TF-IDF score. \n",
    "2. Cosine Similarity: \n",
    "Concept:\n",
    "Measures the similarity between two vectors by calculating the cosine of the angle between them. \n",
    "Calculation:\n",
    "Takes the dot product of the two vectors and divides it by the product of their magnitudes. \n",
    "Interpretation:\n",
    "A cosine similarity value of 1 indicates identical vectors (perfectly similar), 0 indicates orthogonal vectors (no similarity), and -1 indicates completely opposite vectors (perfectly dissimilar). \n",
    "Usage:\n",
    "Used to compare the numerical vectors created by TF-IDF, determining how closely two documents match based on their term frequencies and importance. \n",
    "In summary:\n",
    "TF-IDF creates numerical representations of documents, and cosine similarity compares these representations to determine how similar the documents are based on their content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c135e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def compute_tfidf(df, column):\n",
    "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[column])\n",
    "    return tfidf_matrix, vectorizer\n",
    "\n",
    "# Compute TF-IDF for both datasets\n",
    "bd_tfidf, bd_vectorizer = compute_tfidf(bd_tech_processed, 'combined')\n",
    "ts_tfidf, ts_vectorizer = compute_tfidf(ts_tech_processed, 'combined')\n",
    "\n",
    "# Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_sim_matrix = cosine_similarity(ts_tfidf, bd_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d64965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz, process\n",
    "\n",
    "# Limit to top 5 candidates and apply fuzzy matching\n",
    "tqdm.pandas()\n",
    "fuzzy_matches = []\n",
    "\n",
    "for i in tqdm(range(len(ts_tech_processed)), desc=\"Matching top candidates\"):\n",
    "    ts_text = ts_tech_processed.iloc[i]['combined']\n",
    "    top_indices = cos_sim_matrix[i].argsort()[-5:][::-1]\n",
    "    candidates = bd_tech_processed.iloc[top_indices]['combined'].tolist()\n",
    "    result = process.extractOne(ts_text, candidates, scorer=fuzz.token_set_ratio)\n",
    "    fuzzy_matches.append(result)\n",
    "\n",
    "# Store results\n",
    "ts_tech_processed['fuzzy_match'] = fuzzy_matches\n",
    "\n",
    "# Display sample results\n",
    "ts_tech_processed[['name', 'fuzzy_match']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd0265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract matched product names from fuzzy match results\n",
    "# Assuming each entry in 'fuzzy_match' is a tuple: (matched_text, score)\n",
    "matched_names = ts_tech_processed['fuzzy_match'].dropna().apply(lambda x: x[0])\n",
    "match_scores = ts_tech_processed['fuzzy_match'].dropna().apply(lambda x: x[1])\n",
    "\n",
    "# Create a DataFrame for matched pairs\n",
    "matched_df = ts_tech_clean.loc[matched_names.index, ['name', 'description', 'url', 'categories_ts']].copy()\n",
    "matched_df['matched_to'] = matched_names.values\n",
    "matched_df['match_score'] = match_scores.values\n",
    "\n",
    "# Merge with bd_tech on 'product_name' (which matched_to points to)\n",
    "bd_matched = bd_tech_clean[['product_name', 'description', 'seller_website', 'categories_bd']].copy()\n",
    "bd_matched.columns = ['matched_to', 'bd_description', 'bd_url', 'bd_category']\n",
    "\n",
    "# Join TS matches with BD products\n",
    "merged = pd.merge(matched_df, bd_matched, on='matched_to', how='left')\n",
    "display(merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate matched fields (favor longer description, combine URLs)\n",
    "merged['product_name'] = merged['name']\n",
    "merged['description'] = merged.apply(lambda row: row['description'] if len(str(row['description'])) > len(str(row['bd_description'])) else row['bd_description'], axis=1)\n",
    "merged['url'] = merged.apply(lambda row: row['url'] if pd.notna(row['url']) and row['url'] != '' else row['bd_url'], axis=1)\n",
    "merged['category'] = merged.apply(lambda row: row['category'] if pd.notna(row['category']) and row['category'] != '' else row['bd_category'], axis=1)\n",
    "merged['source'] = 'matched'\n",
    "\n",
    "# Select relevant columns\n",
    "master_matched = merged[['product_name', 'description', 'url', 'category', 'source', 'match_score']]\n",
    "\n",
    "# Unmatched TS entries\n",
    "matched_ts_names = set(matched_df['name'])\n",
    "ts_unmatched = ts_tech[~ts_tech['name'].isin(matched_ts_names)]\n",
    "ts_only = ts_unmatched[['name', 'description', 'url', 'category']].copy()\n",
    "ts_only.columns = ['product_name', 'description', 'url', 'category']\n",
    "ts_only['source'] = 'ts_only'\n",
    "ts_only['match_score'] = None\n",
    "\n",
    "# Unmatched BD entries\n",
    "matched_bd_names = set(matched_df['matched_to'])\n",
    "bd_unmatched = bd_tech[~bd_tech['product_name'].isin(matched_bd_names)]\n",
    "bd_only = bd_unmatched[['product_name', 'description', 'seller_website', 'main_category']].copy()\n",
    "bd_only.columns = ['product_name', 'description', 'url', 'category']\n",
    "bd_only['source'] = 'bd_only'\n",
    "bd_only['match_score'] = None\n",
    "\n",
    "# Combine all parts into master catalogue\n",
    "master_catalogue = pd.concat([master_matched, ts_only, bd_only], ignore_index=True)\n",
    "\n",
    "display(master_catalogue.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312fcf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the info on source=matched data only \n",
    "matched_stats = master_catalogue[master_catalogue['source'] == 'matched'].info()\n",
    "display(matched_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5843a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the master catalogue to a CSV file\n",
    "master_catalogue.to_csv('data/master_catalogue_fuzzy_matching.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8f518",
   "metadata": {},
   "source": [
    "### 3.3. Hyprid feature-based entity resolution with fuzzy token-set ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889aa6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from recordlinkage import Index\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32208c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaad the cleaned ts and bd tech DataFrames\n",
    "bd = pd.read_csv('data/bd_technologies_cleaned.csv')\n",
    "ts = pd.read_csv('data/ts_technologies_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabfb814",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.info(), ts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade31d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block on categories to reduce candidate pairs\n",
    "indexer = Index()\n",
    "indexer.block(left_on='categories_bd', right_on='categories_ts')\n",
    "candidates = indexer.index(bd, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cb0dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each candidate pair, compute fuzzy score on `combined`\n",
    "matches = []\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for bd_idx, ts_idx in tqdm(candidates, desc=\"Blocking + Fuzzy matching\"):\n",
    "    bd_text = bd.at[bd_idx, 'combined']\n",
    "    ts_text = ts.at[ts_idx, 'combined']\n",
    "    score = fuzz.token_set_ratio(bd_text, ts_text)\n",
    "    if score >= 85:\n",
    "        matches.append((bd_idx, ts_idx))\n",
    "        scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196ffa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a DataFrame of matched index‚Äêpairs + score\n",
    "matches_df = pd.DataFrame(matches, columns=['bd_idx','ts_idx'])\n",
    "matches_df['match_score'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248cdf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract matched rows, reset index to align with matches_df\n",
    "bd_matched = bd.loc[matches_df['bd_idx']].reset_index(drop=True)\n",
    "ts_matched = ts.loc[matches_df['ts_idx']].reset_index(drop=True)\n",
    "bd_matched.index = matches_df.index\n",
    "ts_matched.index = matches_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994bf475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuse matched rows into one DataFrame (suffixing columns)\n",
    "matched_full = pd.concat([\n",
    "    bd_matched.add_suffix('_bd'),\n",
    "    ts_matched.add_suffix('_ts')\n",
    "], axis=1)\n",
    "matched_full['match_score'] = matches_df['match_score']\n",
    "matched_full['source'] = 'matched'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adcfb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare TS-only records\n",
    "ts_unmatched_idx = ts.index.difference(matches_df['ts_idx'])\n",
    "ts_unmatched = ts.loc[ts_unmatched_idx].reset_index(drop=True)\n",
    "ts_only = ts_unmatched.add_suffix('_ts')\n",
    "ts_only['match_score'] = pd.NA\n",
    "ts_only['source'] = 'ts_only'\n",
    "# add blank BD columns\n",
    "for col in bd.columns:\n",
    "    ts_only[col + '_bd'] = pd.NA\n",
    "    \n",
    "# Prepare BD-only records\n",
    "bd_unmatched_idx = bd.index.difference(matches_df['bd_idx'])\n",
    "bd_unmatched = bd.loc[bd_unmatched_idx].reset_index(drop=True)\n",
    "bd_only = bd_unmatched.add_suffix('_bd')\n",
    "bd_only['match_score'] = pd.NA\n",
    "bd_only['source'] = 'bd_only'\n",
    "# add blank TS columns\n",
    "for col in ts.columns:\n",
    "    bd_only[col + '_ts'] = pd.NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e5044a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder TS-only and BD-only to match matched_full columns\n",
    "all_cols = list(matched_full.columns)\n",
    "ts_only = ts_only[all_cols]\n",
    "bd_only = bd_only[all_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3d04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate into final master catalogue\n",
    "master_catalogue_hybrid = pd.concat(\n",
    "    [matched_full, ts_only, bd_only],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# eorder columns: put product identifiers front\n",
    "cols = (\n",
    "    ['product_name_bd','name_ts']\n",
    "    + [c for c in all_cols if c not in ('product_name_bd','name_ts')]\n",
    ")\n",
    "master_catalogue_hybrid = master_catalogue_hybrid[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_catalogue_hybrid.to_csv('master_catalogue_hybrid.csv', index=False)\n",
    "display(\"Master catalogue shape:\", master_catalogue_hybrid.shape)\n",
    "display(master_catalogue_hybrid.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display master catalogue with matched entries and the info\n",
    "master_catalogue_hybrid[master_catalogue_hybrid['source'] == 'matched'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9c6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "{dragonfly_env}",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
